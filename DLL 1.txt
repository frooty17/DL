# -*- coding: utf-8 -*-
"""DL1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zrY9fR5nJPUf1yDtT9A6K7cgqJHKNqIZ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
import keras
from keras.layers import Dense
from keras.models import Sequential
import warnings
warnings.filterwarnings("ignore")

from google.colab import files
uploaded = files.upload()

# Load the dataset
data = pd.read_csv('Boston.csv')

# Display the first few rows of the dataset
print(data.head())

# Display the shape of the dataset
print(data.shape)

# Display the data types of each column
print(data.dtypes)

# Check for missing values
print(data.isnull().sum())

# Display summary statistics of the dataset
print(data.describe())

#Data Visualization
sns.displot(data['medv'])

# Data Visualization
sns.displot(data['medv'])

correlation = data.corr()
print(correlation['medv'])

fig, axes = plt.subplots(figsize=(15, 12))
sns.heatmap(correlation, square=True, annot=True)

# Splitting Data into testing and training data
X = data.drop('medv', axis=1) # Replace 'PRICE' with 'medv'
y = data['medv'] # Replace 'PRICE' with 'medv'
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 4)

# Normalizing the data
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Model Building
model = Sequential()
model.add(Dense(128, activation='relu', input_dim=14))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')
model.summary()

print(X_train.shape)
print(X_train.shape[1])

# Fitting the data to the model
model.fit(X_train, y_train, epochs=100)

